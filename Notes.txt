This document includes the development notes of the PrOMMiS LCA Integration code

The flow includes 5 main tasks:
1- Extract inputs/outputs from PrOMMiS
2- Communicate PrOMMiS resuts with openLCA
3- Run openLCA model
4- Extract openLCA results
5- Pass openLCA results to PrOMMiS

Step 1: Extract inputs/outputs from PrOMMiS
--------------------------------------------
This step is performed entirely in prommis_LCA_data.py

    Step 1.1: Identify all LCA-relevant flows in the UKy PrOMMiS flowsheet
    ======================================================================
In total, 17 material streams with 45 material flows were identified in addition to 4 electricity flows and 2 heat flows. 
2 new flows were also added to reflect data from the NETL UP Library: sodium hydroxide and oxalic acid.

    Step 1.2: Extract all flows/parameters and structure them in a dataframe
    =========================================================================

The end goal of this step is to have all relevant flows structured as follow:

| Flow_ID | Flow        | In/Out |   Category   | Amount 1 | Unit 1 | Amount 2 | Unit 2 |
|---------|-------------|--------|--------------|----------|--------|----------|--------|
|    1    | Pump power  |   In   |  Electricity |  12      | kW     |          |        |
|    2    | Li product  |   Out  |  Product     |  136     | kg/hr  |          |        |
|    3    | Feed - MatX |   In   |  Material    |  100     | m3/hr  |  1.7     | kg/m3  |

This structure is passed into a csv file using prommis_LCA_data.py, which runs the UKy flowsheet and extracts the relevant LCA data.


Step 2: Communicate PrOMMiS results with openLCA
------------------------------------------------

    Step 2.1: Develop function to convert PrOMMiS results to LCA-relevant results
    ============================================================================
This step is performed in prommis_LCA_conversions.py. The main function in the script, convert_flows_to_lca_units,
adds two new columns to the dataframe from step 1: LCA Amount and LCA Unit (usually kg, m3, L, kWh, or MJ). This
function uses the Pyomo framework and a robust error-handling system to convert any common unit to LCA units.
It also uses pubchempy and pymatgen to convert moles to kg for any common chemical name.
It returns the new df and creates a new csv file.

    Step 2.2: Develop function that evaluates PrOMMiS flows and converts them to LCA-relevant flows normalized to selected FU.
    =========================================================================================================================
This step is performed in finalize_LCA_flows.py. The main() function utilizes the merge_flows and finalize_df functions to convert
the LCA information into a new dataframe which is ready to be imported into openLCA. The main function completes the following steps:

1. Imports the LCA csv from the previous step
2. Uses the merge_flows function to:
    a. Combine the REO feed streams into one single feed stream, "374 ppm REO Feed"
    b. Combine the REO product streams into one single product stream, "99.85% REO Product"
3. Enters the dataframe and the desired reference flow into the finalize_df function to:
    a. Convert all LCA values based on the functional unit/reference flow using the convert_to_functional_unit function
    b. Create a new dataframe with only 7 columns: Flow_Name, LCA_Amount, LCA_Unit, Is_Input, Reference_Product, Flow_Type, and Description
    c. Combine identical flows into one value using the merge_duplicate_flows function
4. Returns this new dataframe and creates a new csv file

    Step 2.3: Pass converted PrOMMiS outputs to openLCA
    ===================================================

    The code in this step involves several steps that mimic the process creation in openLCA
    1- Use olca ipc to connect to openLCA
    2- Create a new unit process and prompt the user to enter its metadata (e.g., name, description, etc.)
    3- Read the dataframe produced in steps 2.1 and 2.2
    4- A function loops through the dataframe row by row and for each row creates a flow to be entered in the unit process
            - The function first creates an empty process
            - then it creates an exchange for the reference product
                - Here the user is given two options:
                    * Option 1: create exchange for quantitatve reference from an existing flow. If this is selected - the user has to:
                        a- enter a keyword --> the user is provided with a list of product flows available in the database (connected via IPC)
                        b- the user selects a flow
                        c- an exchange is created
                    * Option 2: create new flow, use it to create exchange, and set it as quantitative reference
            - then for each row in the given dataframe
                - if the flow is ELEMENTARY_FLOW --> the function automatically creates an exchange for it using a predefined uuid
                    * Our work includes a dictionary for elementary flows in FEDEFL with their uuids
                - if the flow is product or waste flow
                    * the user enters a keyword used to fetch given flows
                    * the user selects a flow from a given convert_flows_to_lca_units
                    * the function searches for processes that provide the given flow
                    * the user select a process
                    * FUNCTION MOVES TO THE NEXT ROW
        For all the steps above, the flow amount, unit, input/output are all retrieved from the given df generated in steps 2.1-2.2

    #########################################################################
    # FOR FUTURE REFERENCE - TODO'S IN THE ABOVE STEPS
        High priority:
        --------------
        - Compile inventory for missing chemicals in our database and create processes for them
        - [In Progress]: Create a impact assessment method that calculates GWP, CED, and Water consumption - and add it to the database

        Low priority:
        -------------
        - Provide the user with the option of changing their keyword

    #########################################################################

Step 3: Run openLCA model
-------------------------

Running the analysis in openLCA requires building the analysis setup
we should create a calculationsetup object and then define its attributes:
    - allocation --> if omitted then default allocation is used. In this version of the code, default allocation will be used.
    - amount: FU amount --> if omitted, the quantitative reference amount is used
    - impact_method --> if omitted, we don't get a LCIA but rather only a LCI
    - normalization and weighting set
    - parameters --> specific run parameter
    - target: this is what we're calculating --> this takes a product system object
    - unit --> overrides FU
    - with_costs: includes cost calculation
    - with regionalization --> enables regionalized LCA

To run the analysis, the jupyter notebook user should use the run_analysis function which takes in three main arguments
    - the client (netl)
    - the product system uuid
    - the impact assessment method uuid

Here it's important to note that the method uuid will be pre-defined in the jupyter notebook.
In this project we are attaching a openLCA database that contains:
    - the needed libraries/processes to evaluate the PrOMMiS flowsheets
    - a impact assessment method that evaluates water consumption, CED, and global warming potential

Step 4: Extract openLCA results
-------------------------------

Step 3 returns a result object that can be used to extract the LCIA results

In this step two sets of results are extracted for each impact category (GWP, CED, WC)
    - Total impact: The total impacts are calculated using the generate_total_results function in the generate_total_results.py module.
                    This function uses get_total_impacts() attribute of a olca_schema object

    - Impacts by category:  This is also referred to as 'contribution tree' in openLCA.
                            To get the contribution tree, we use the generate_contribution_tree function from the generate_contribution_tree.py module
                            This method heavily relies on the utree module from olca_ipc library
                            This method requires the user to determine the number of nodes and levels.
                            Nodes are also referred to as the child nodes of a process like electricity, hear, sulfuric acid, etc. (e.g., impact by category)
                            Levels represent the number of step away from the main product.
                            Few important notes on this function and its application
                                a- setting nodes to (-1) reports all the nodes possible --> should be recommended in the analysis
                                b- setting levels to (1) reports the total impact for each node (no ramifications)


Step 5: Pass openLCA results to PrOMMiS
---------------------------------------


Database Development notes
--------------------------
A database is developed with the required data and methods to create an openLCA model for
the PrOMMiS UKy flowsheet.

1- Regarding data, the following libraries are used:
        - USLCI
        - Separate UPs that are not found in the Federal LCA Commons are exported from the NETL master database (currently still under development)
    Additionally, two inputs from the UKy flowsheet inventory are not available, namely: 1) oxalic acid and 2) D2EHPA
    To address this data limitation, we compile inventory and develop UPs for the missing inputs in openLCA
    The oxalic acid inventory is retrieved from ecoinvent database and the inventory for DEHPA is collected from the
    literature (https://doi.org/10.1016/j.resconrec.2022.106689)

2- Regarding methods, a new method 'PrOMMiS Impact Assessment Method' is developed that encompasses several impact categories
    - Acidification Potential                                       <- imported from TRACI 2.1
    - Cumulative Energy Demand                                      <- developed by compiling all the characterization factors from the Federal LCA Commons CED method
    - Cumulative Energy Demand - Non-renewable                      <- developed by only including the characterization factors for non-renewable resources
    - Cumulative Energy Demand - renewable                          <- developed by only including the characterization factors for renewable resources
    - Eutrophication Potential                                      <- imported from TRACI 2.1
    - Global Warming Potential [AR6, 100 yr]                        <- imported from TRACI 2.1
    - Global Warming Potential [AR6, 20 yr]                         <- imported from TRACI 2.1
    - Human Health - cancer                                         <- imported from TRACI 2.1
    - Human Health - non-cancer                                     <- imported from TRACI 2.1
    - Human Health - particulate matter                             <- imported from TRACI 2.1
    - Ozone Depletion Potential                                     <- imported from TRACI 2.1
    - Smog Formation Potential                                      <- imported from TRACI 2.1
    - Water Consumption (NETL)                                      <- developed by NETL - imported from TRACI 2.1 (NETL)


Questions and future considerations
------------------------------------
1- Is hotspot analysis a part of the final outcome?
    If not then the current model structure is finalize_df
    If yes, then this would require modeling the UKy flowsheet (for example)
    using process-based LCA rather than aggregating all the inventory in one unit process
    In the second case (yes), we might need to do minor changes to the model structure to
    allow modeling each process unit separately (something to keep in mind: in the case of
    processes producing co-products/by-products, this becomes exponentially more complex to
    solve - but definitely doable)

2- When creating processes, is it better to search for the process/
provider and then select a flow? Or is it better to select a flow and then search for relevant
providers (which is the current approach)?

3- Potential improvement in the future: the user gets to see a list of impact assessment methods
and select one for their analysis.

4- In the process description step, we can ask the user to enter different LCA descriptors and
then combine those to form the process description. This ensures the process has all the necessary
information: goal, scope, functional unit, assumptions, etc.

5- dislpay units in the list of flows

6- enable modifying flow unit (in lca_df_finalized) and adding conversion parameters
where needed when creating a flow (e.g., natural gas - from MJ to kg)


New Technical Direction - EY25 - Q3/Q4 (11/10/2025)
---------------------------------------------------\

Info from TD
    * Establish a protocol for bi-directional communication between 
            PrOMMiS and openLCA - allow openLCA results to inform PrOMMiS optimization
                ** Supports constraint-based decision framework
                    My understanding, is that this should involve something simialr to the 
                    current PrOMMiS optimization framework
                        1- Setting Goal
                        2- Setting optimization parameter
                        3- Setting constraints
                        2 and 3 are probably what the TD note is trying to address
                            (e.g. "narrow the analysis scope in the context of practical considerations)
    * Deliverable should be in the form of a 
        ** methods memo
        ** demonstration on the UKy flowsheet
        ** test on other flowsheets

Notes on optimization model in PrOMMiS
    * Optiomization in prommis is done using nonlinear programming via IPOPT which
        ** updates unfixed design variables 
        ** to minimize the active Objective (e.g., minimize environmental impact)

    * the workflow consists of 
        ** unfixing decision variables (e.g., membrane length)
            {variable}.unfix()
        ** adding constraints using pyomo's 
            *** Expression({insert expression})
            *** Param(initialize={insert parameter}, mutable = True)
            *** Constraint(expr={insert conditional expression})
        ** adding objective
            Objective(expr = {insert variable such as cost}, sense = {e.g., minimize})

    * the above optimization elements can all be defined in a single function
        ** for exampple def build_optimization(...)
            to facilitate configuring the optimizer
        ** or this can be a populated automatically based on inputs in a 
            yaml configuration file

Potential additions/modifications to existing solution from previous PrOMMiS-LCA integration example
    * Create new function to configure the optimization solver
        variables, constraints, Objective
        ** the objective and constraint in this case can be complicated 
            *** which impact category?
            *** is cost included?
            *** if more than one impact category 
                weighing factors might be needed for the optimization equation
            *** if env impact + cost are included
                weighing factors might be needed for the optimization equation 
    * Add feature to create a parameter for each exchange
    * Create function to populate parameters after every iteration
        ** we can possibly include a feature - maybe an option for the user - to save 
            all the solutions for subsequent revision and/or data analysis
    * Create function to set allocation - which is very crucial for CM flowsheets
    * Create function to allow modeling displaced/avoided products
    * Modify existing script to allow the user define uncertainty distributions to exchanges
        Potential applications include
        ** Optimizing based on a statistical inference rather than a single deterministic value
        ** Allowing the user to run an uncertainty analysis for the final solution

Other necessary modifications to facilitate the reusability of the solution for other flowsheets
    * PrOMMiS data collection and conversion to LCA relevant units
        ** script should be limited to PrOMMiS output
        ** add function to enable the user to add exchanges as needed when running the model
            (without having to modify the code)
    * Exchange table development
        ** modify existing functions or create new function that allows users to add documentation for each
            exchange

Foqus installation on Ubuntu
----------------------------
1. Install conda
    `wget https://repo.anaconda.com/archive/Anaconda3-2025.12-1-Linux-x86_64.sh`
    `bash Anaconda3-2025.12-1-Linux-x86_64.sh`
    (Run as user; installs to ~/anaconda3; accept .bashrc edits)
2. Setup conda
    conda create -n ccsi-foqus python=3.12
    conda activate ccsi-foqus
    conda install pyqt
3. Install FOQUS
    git clone https://github.com/CCSI-Toolset/FOQUS
    cd FOQUS
    pip install -e .
4. Install extras: Psuade
    conda install --yes -c conda-forge -c CCSI-Toolset psuade-lite=1.9
5. Install extras: NLopt
    conda install -y -c conda-forge nlopt
6. Install PrOMMiS
    pip install prommis  (note: downgrades NumPy to 1.26.4
    idaes get-extensions
    (note: I did not need to reinstall pyomo)
7. Install extras: IPOPT
    conda install -y -c conda-forge ipopt
8. Install older version of PyMatGen
    pip install pymatgen==2024.1.26
9. Install prommis-lca
    git clone https://github.com/keylogiclca/lca-prommis.git
    cd lca-prommis
    pip install -e .
10. Install missing dependencies for Qt (to run FOQUS GUI)
    sudo apt install -y libxcb-xinerama0 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-randr0 
        libxcb-render-util0 libxcb-shape0 libxcb-xfixes0 libxkbcommon-x11-0 libglu1-mesa
    (NOTE: only the last two are mentioned in FOQUS readthedocs)

    Note:
    ====
    * openLCA-related libraries require python >= 3.11 
        this was the main reason to move to linux/wsl
        we can run prommis/foqus/olca in python 3.12
    * After resolving all dependency issues - one remained
        * foqus     requires    matplotlib <=3.7.5 and >=3.6.0
        * pymatgen  requires    matplotlib >=3.8

        * Only solution --> downgrade matplotlib to 3.7.5
                        --> downgrade pymatgen to 2022.x 
                            to be compatible with matplotlib 3.7.5 

                            Issue: pymatgen 2022.x requires Python 3.10

    BUT - since we're not plotting anything with Pymatgen - this dependency 
        issue shouldn't cause any problems 
    
    SKIP THE DETAILS --> see final versions below
    ================
    matplotlib  3.10.7
    pymatgen    2024.11.13
    tabulate    0.9.0

11. Install openLCA for Linux 
    =========================
    11.1 wget https://share.greendelta.com/index.php/s/hcl5JAB1p0FxfFe/download
    11.2 mv download openLCA_mkl_Linux_x64_2.5.0_2025-06-16.tar.gz
    11.3 gunzip openLCA_mkl_Linux_x64_2.5.0_2025-06-16.tar.gz
    11.4 tar -xvf openLCA_mkl_Linux_x64_2.5.0_2025-06-16.tar
    11.5 mkdir Downloads
    11.6 mv openLCA_mkl_Linux_x64_2.5.0_2025-06-16.tar Downloads/

12. Test run openLCA; it works!
    ==========================
    12.1 cd openLCA/
    12.2 ./openLCA

13. Use and testing
    ===============

- Running foqus: 
    * Now that conda automatically starts - cd ccsi-foqus
    * run:  foqus
        or
            foqus &     #this allows the user to regain control of their terminal while using foqus 

14. Testing
    =======
    * Go to flowsheets (2nd tab on the top)
    * Create a node in foqus - Left toolbar - 2nd button from the top
    * Select the 'Select' option (1st button on Left toolbar) 
    * Click on node
    * Edit node (3rd button from the bottom in left toolbar)
    * Create new output variable - call it 't' and initiate it at '0' 
    * Add a script that calculates the output variable - Use the below script:
    
    """
        import pandas as pd
        import olca_schema as olca
        from netlolca.NetlOlca import NetlOlca
        import prommis.uky.uky_flowsheet as uky 
        import src as lca_prommis

        m, _ = uky.main() 

        prommis_data = lca_prommis.data_lca.get_lca_df(m)

        df = lca_prommis.convert_lca.convert_flows_to_lca_units(prommis_data, hours=1, mol_to_kg=True, water_unit='m3')

        REO_list = [
            "Yttrium Oxide",
            "Lanthanum Oxide",
            "Cerium Oxide",
            "Praseodymium Oxide",
            "Neodymium Oxide",
            "Samarium Oxide",
            "Gadolinium Oxide",
            "Dysprosium Oxide",
        ]

        df = lca_prommis.final_lca.merge_flows(df, merge_source='Solid Feed', new_flow_name='374 ppm REO Feed', value_2_merge=REO_list)


        df = lca_prommis.final_lca.merge_flows(df, merge_source='Roaster Product', new_flow_name='73.4% REO Product')


        df = lca_prommis.final_lca.merge_flows(df, merge_source='Wastewater', new_flow_name='Wastewater', merge_column='Category') 

        df = lca_prommis.final_lca.merge_flows(df, merge_source='Solid Waste', new_flow_name='Solid Waste', merge_column='Category') 

        finalized_df = lca_prommis.final_lca.finalize_df(
                df=df, 
                reference_flow='73.4% REO Product', 
                reference_source='Roaster Product',
                water_type='raw fresh water'
            )

        f["t"] = finalized_df["LCA_Amount"].iloc[0]
    """
    * Running the script should update the value of the output 't' from 0 to 1043142.7399884746

LCA-PrOMMiS Development Notes - (Q3Q4)
--------------------------------------

12/10/2025
==========
    - Anuja (from Miguel's Team) gave an introduction to FOQUS
    - After several iterations, we were able to run foqus on Ubuntu

    Next steps
    - Reach out to Anuja and ask for another meeting to explain how DFO can be used?
        (Now that FOQUS, PrOMMiS, and LCA are all set up in one working environment)

    - Depending on the outcome - we reach out again to NETL with the updated approaches


    - Brainstorming on FOQUS use

        PrOMMiS Node
        ------------
        * Outputs (Relevant to LCA)
            Conduct all calculations in output code snippet 
            Convert values to LCA relevant inputs
            The outputs should be the different exchanges values   
                - separately
                    Or
                - df
            Can we pass an object like a df via foqus?
                Tried testing it by setting the variabel type to Object 
                but it's not working
            
            solution 1
                * Export finalized_df as csv to a temp folder
                * import that same csv in the openLCA node

            solution 2
                * Extract the distinct exchange values from prommis node
                * pass the values to the openLCA node
                * compile values again into a df that can be used to create new 
                    process, PS, and results
        
            After discussion with Anuja - solution 1 doesn't work
            There has to be parameters being passed via an edge
        
        openLCA node
        ------------
            * Works perfectly 
            * All the work and user input is done via the Ubuntu terminal

    Update 12/15/2025
    -----------------
    - Next steps: Incroporate Prommis, LCA calculations, and the optimizer
        * for context:see the discussion notes with Anuja saved in the project folder 
                    Meetings/Other Meetings/2025-12-12/20251212_MoM.docx
        * PrOMMiS node - the modifications are highlighted below
            ** inputs
                - I created two inputs for the variables i want to vary throughout the optimization exercise
                    In this case I used the leach liquid feed and the recycle split fraction
            ** outputs
                - the outputs from this node are the lca prommis data converted to LCA relevant units
                - every exchange had a unit
                    For future reference - to improve the code, the output variables should be called the 
                        same as the final lca_df_finalized csv file 
                        (this will make more sense as you read the documentation)
            ** Node script
                - The script includes the code to import the dependencies, run the prommis uky flowsheet
                    extract the data, convert it to LCA relevant units, and pass it to the output variables
                    created in the step above
                - One important aspect in the node script is the setup of the inputs (Dependent variables)
                    There were two ways to incorporate these variables
                        Option 1: modify the uky main function and set those as inputs (e.g., main (uky, leach_liquid, split_fraction))
                                    and then when calling this function in the node script, the second and third arguments would be 
                                    set to be equal to the foqus inputs - because after every iteration, those will change and we want them to 
                                    be passed back to prommis
                                
                                The challenge with option 1 is that every time we want to run a simulation we have to:
                                    1) modify the prommis uky main function
                                    2) reinstall or basically upgrade prommis in the ccsi-foqus environment
                                    3) run foqus and run optimization exercise
                                        NOTE: If there are errors/mistakes in the edits, debugging can be challenging as it will
                                            probably require repeating the 3 steps above until the errors disappear
                        Option 2: Execute all the steps included in the uky main function
                                    Among these steps, the user can set the selected parameters equal to the foqus dependent variables
                                    ============
                                    Code snippet
                                    ============
                                    m = uky.build()
                                    uky.set_operating_conditions(m)
                                    # Important note regarding the below - all the variables in the UKy flowsheet are already fixed
                                    # setting the leach liquid feed flow in prommis to the appropriate input variable in foqus
                                    # this automatically unfixes the variable in PrOMMiS 
                                    if "Leach liquid feed" in x:
	                                    m.fs.leach_liquid_feed.flow_vol.fix(x["Leach liquid feed"])
                                    # setting the split fraction in prommis to the appropriate input variable in foqus
                                    if "split_fraction" in x:
	                                    m.fs.load_sep.split_fraction[0.0, 'recycle'].fix(x["split_fraction"])
                                    ==========================================================================
                                    NOTE: You have to import all the dependencies in the module containing the UKy main function
                                    this option was used in this exercise
        * openLCA node
            ** inputs
                - For each output from the 'PrOMMiS' node, an input was created in the openLCA node
                - the inputs were given the following
                    * 0 minimum value
                    * 1000000 maximum value (for the sake of this exercise)
                    * 0 initial value
                - For future reference, the inputs should be called the same as the PrOMMiS outputs 
                    and the same as the lca_df_finalized csv file generated inthe PrOMMiS node
                    (this will make more sense when you get to the Node script part below)
            ** outputs
                - Originally before meeting Anuja, the output was set to be multiple impact categories
                - In this exercise, only GWP was used (This will make more sense when u read the next point (Node Script))
            ** Node Script
                - At this point (as of 12/15/2025), we didn't have the code needed to set parameters in openLCA and
                    update them at every iteration
                - For this reason, the openLCA calculation was bypassed by a quick (not ideal) formula to calculate the total GWP
                    * We needed the exchanges table
                        * the amounts were retrieved from the PrOMMiS node via the edge
                        * a python dictionary was hard-coded to connect each flow to its
                            unit, is_input, is_reference, etc.
                            NOTE: If the names were matching everywhere, it would have been easy to automate the generation
                            of the python dictionary via a separate function # at this point some of the above notes should make more sense
                        * The gwp factor for each input/emission was added (hard coded) into the dictionary
                    * A script was drafted to multiply the inputs and emissions amounts with their corresponding GWP factors
                - the calculated GWP was returned to the openLCA node output f(["GWP"])
                ============
                Code snippet
                ============
                def get_amount(v):
                    return float(v.value) if hasattr(v, "value") else float(v)

                total_gwp = 0.0

                for param, meta in param_metadata.items():
                    if "gwp" not in meta:
                        continue
                    total_gwp += get_amount(x[param]) * meta["gwp"]
                # store the final value in the foqus output
                f["GWP"] = total_gwp
                ==============================================================
        * Edge: Create an edge connecting PrOMMiS to the openLCA function 
                when configuring that edge create a connection between the outputs of PrOMMiS node and the inputs of openLCA
                Check:
                    Go to the openLCA node and make sure all the inputs are highlighted in 'pink' => connected

        * Optimization tab (at the very top of the GUI)
            ** Variables
                - You will automatically find the two UKy input variables you defined above
                - You need to change them to 'Decision' variables to allow the optimizer to run
                    NOTE: THE OPTIMIZER REQUIRES AT LEAST TWO DECISION VARIABLES
                - Select reasonable range (min/max) and initial value
                    Observation: When i set very wide ranges - many of the runs failed, which is probably expected as
                                many of the extreme sets of split_fraction and leach feed volume didnt make any sense

            ** Objective
                - Create a new objective - set it to f["openLCA"]["GWP"]
                    NOTE: You can use the variable explorer for this
                - give it a penalty scale
                    TODO:   how to determine this factor
                            What's its effect on the optimization results
                - give it a 'Variable for Failure' - I set this as the static run result
            
            ** Solver
                - Select BOBYQA as an optimization model
            ** Run
                - Run 

    Update 12/16/2025: Constructing an optimization flosheet without the GUI
    -----------------
    It seems there is a way to setup foqus and DFO directly through python based on the developer documentation
        *  https://foqus.readthedocs.io/en/stable/chapt_dev/index.html#
    
    Going through the foqus git repository: https://github.com/CCSI-Toolset/FOQUS.git
    there is a possibility of using foqus features directly from foqus_lib.framework
    In this repository, some modules of interest include:
        * foqus_lib.framework.graph --> Contains several modules - most notably for our applicationa/use:
          =========================
            1- A 'graph.py' module containing methods to:
                - add/delete/edit nodes: addNode, deleteNode, deleteNodes, renameNode
                - add/delete edges: addEdge, deleteEdge, deleteEdges
                - run a node: runNode
                - run all nodes in the graph: runGraph
                - do other sanity checks on the graph

                Link: https://github.com/CCSI-Toolset/FOQUS/blob/2d5dc0eb824e442c3afa97e3a9a756e4f79094a4/foqus_lib/framework/graph/graph.py#L857

            2- A 'node.py' module containing all the classes for nodes, most important is the 'Node' class
                - Node --> stores all the information for graph nodes, run calculations/simulations etc.

            3- A 'edge.py' module containing two main classes
                - Edge connect: stores information about a connection between two variables
                - Edge: manage edges --> includes functions to:
                    - add Edge
                    - Remove Edge
                    - Transfer information

        * foqus_lib.framework.optimizer --> Contains several modules for different solvers
          =============================
            This module contains a sub-module for NLopt which we use in our LCA-PrOMMis integration work
            1- NLopt.py
                Contains a single class -- 'opt' which enables:
                    - Initializing NLopt including setting the solver (e.g., BOBYQA) among many other variables
                        (In the exercise with Anuja we kept those unchaged/default)
                    - Running the solver: optimize()
