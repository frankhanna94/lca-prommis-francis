This document includes the development notes of the PrOMMiS LCA Integration code

The flow includes 5 main tasks:
1- Extract inputs/outputs from PrOMMiS
2- Communicate PrOMMiS resuts with openLCA
3- Run openLCA model
4- Extract openLCA results
5- Pass openLCA results to PrOMMiS

Step 1: Extract inputs/outputs from PrOMMiS
--------------------------------------------
This step is performed entirely in prommis_LCA_data.py

    Step 1.1: Identify all LCA-relevant flows in the UKy PrOMMiS flowsheet
    ======================================================================
In total, 17 material streams with 45 material flows were identified in addition to 4 electricity flows and 2 heat flows. 2 new flows were also added to reflect data from the NETL UP Library: sodium hydroxide and oxalic acid.

    Step 1.2: Extract all flows/parameters and structure them in a dataframe
    =========================================================================

The end goal of this step is to have all relevant flows structured as follow:

| Flow_ID | Flow        | In/Out |   Category   | Amount 1 | Unit 1 | Amount 2 | Unit 2 |
|---------|-------------|--------|--------------|----------|--------|----------|--------|
|    1    | Pump power  |   In   |  Electricity |  12      | kW     |          |        |
|    2    | Li product  |   Out  |  Product     |  136     | kg/hr  |          |        |
|    3    | Feed - MatX |   In   |  Material    |  100     | m3/hr  |  1.7     | kg/m3  |

This structure is passed into a csv file using prommis_LCA_data.py, which runs the UKy flowsheet and extracts the relevant LCA data.


Step 2: Communicate PrOMMiS results with openLCA
------------------------------------------------

    Step 2.1: Develop function to convert PrOMMiS results to LCA-relevant results
    ============================================================================
This step is performed in prommis_LCA_conversions.py. The main function in the script, convert_flows_to_lca_units,
adds two new columns to the dataframe from step 1: LCA Amount and LCA Unit (usually kg, m3, L, kWh, or MJ). This
function uses the Pyomo framework and a robust error-handling system to convert any common unit to LCA units.
It also uses pubchempy and pymatgen to convert moles to kg for any common chemical name.
It returns the new df and creates a new csv file.

    Step 2.2: Develop function that evaluates PrOMMiS flows and converts them to LCA-relevant flows normalized to selected FU.
    =========================================================================================================================
This step is performed in finalize_LCA_flows.py. The main() function utilizes the merge_flows and finalize_df functions to convert
the LCA information into a new dataframe which is ready to be imported into openLCA. The main function completes the following steps:

1. Imports the LCA csv from the previous step
2. Uses the merge_flows function to:
    a. Combine the REO feed streams into one single feed stream, "374 ppm REO Feed"
    b. Combine the REO product streams into one single product stream, "99.85% REO Product"
3. Enters the dataframe and the desired reference flow into the finalize_df function to:
    a. Convert all LCA values based on the functional unit/reference flow using the convert_to_functional_unit function
    b. Create a new dataframe with only 7 columns: Flow_Name, LCA_Amount, LCA_Unit, Is_Input, Reference_Product, Flow_Type, and Description
    c. Combine identical flows into one value using the merge_duplicate_flows function
4. Returns this new dataframe and creates a new csv file

    Step 2.3: Pass converted PrOMMiS outputs to openLCA
    ===================================================

    The code in this step involves several steps that mimic the process creation in openLCA
    1- Use olca ipc to connect to openLCA
    2- Create a new unit process and prompt the user to enter its metadata (e.g., name, description, etc.)
    3- Read the dataframe produced in steps 2.1 and 2.2
    4- A function loops through the dataframe row by row and for each row creates a flow to be entered in the unit process
            - The function first creates an empty process
            - then it creates an exchange for the reference product
                - Here the user is given two options:
                    * Option 1: create exchange for quantitatve reference from an existing flow. If this is selected - the user has to:
                        a- enter a keyword --> the user is provided with a list of product flows available in the database (connected via IPC)
                        b- the user selects a flow
                        c- an exchange is created
                    * Option 2: create new flow, use it to create exchange, and set it as quantitative reference
            - then for each row in the given dataframe
                - if the flow is ELEMENTARY_FLOW --> the function automatically creates an exchange for it using a predefined uuid
                    * Our work includes a dictionary for elementary flows in FEDEFL with their uuids
                - if the flow is product or waste flow
                    * the user enters a keyword used to fetch given flows
                    * the user selects a flow from a given convert_flows_to_lca_units
                    * the function searches for processes that provide the given flow
                    * the user select a process
                    * FUNCTION MOVES TO THE NEXT ROW
        For all the steps above, the flow amount, unit, input/output are all retrieved from the given df generated in steps 2.1-2.2

    #########################################################################
    # FOR FUTURE REFERENCE - TODO'S IN THE ABOVE STEPS
        High priority:
        --------------
        - Compile inventory for missing chemicals in our database and create processes for them
        - [In Progress]: Create a impact assessment method that calculates GWP, CED, and Water consumption - and add it to the database

        Low priority:
        -------------
        - Provide the user with the option of changing their keyword

    #########################################################################

Step 3: Run openLCA model
-------------------------

Running the analysis in openLCA requires building the analysis setup
we should create a calculationsetup object and then define its attributes:
    - allocation --> if omitted then default allocation is used. In this version of the code, default allocation will be used.
    - amount: FU amount --> if omitted, the quantitative reference amount is used
    - impact_method --> if omitted, we don't get a LCIA but rather only a LCI
    - normalization and weighting set
    - parameters --> specific run parameter
    - target: this is what we're calculating --> this takes a product system object
    - unit --> overrides FU
    - with_costs: includes cost calculation
    - with regionalization --> enables regionalized LCA

To run the analysis, the jupyter notebook user should use the run_analysis function which takes in three main arguments
    - the client (netl)
    - the product system uuid
    - the impact assessment method uuid

Here it's important to note that the method uuid will be pre-defined in the jupyter notebook.
In this project we are attaching a openLCA database that contains:
    - the needed libraries/processes to evaluate the PrOMMiS flowsheets
    - a impact assessment method that evaluates water consumption, CED, and global warming potential

Step 4: Extract openLCA results
-------------------------------

Step 3 returns a result object that can be used to extract the LCIA results

In this step two sets of results are extracted for each impact category (GWP, CED, WC)
    - Total impact: The total impacts are calculated using the generate_total_results function in the generate_total_results.py module.
                    This function uses get_total_impacts() attribute of a olca_schema object

    - Impacts by category:  This is also referred to as 'contribution tree' in openLCA.
                            To get the contribution tree, we use the generate_contribution_tree function from the generate_contribution_tree.py module
                            This method heavily relies on the utree module from olca_ipc library
                            This method requires the user to determine the number of nodes and levels.
                            Nodes are also referred to as the child nodes of a process like electricity, hear, sulfuric acid, etc. (e.g., impact by category)
                            Levels represent the number of step away from the main product.
                            Few important notes on this function and its application
                                a- setting nodes to (-1) reports all the nodes possible --> should be recommended in the analysis
                                b- setting levels to (1) reports the total impact for each node (no ramifications)


Step 5: Pass openLCA results to PrOMMiS
---------------------------------------


Database Development notes
--------------------------
A database is developed with the required data and methods to create an openLCA model for
the PrOMMiS UKy flowsheet.

1- Regarding data, the following libraries are used:
        - USLCI
        - Separate UPs that are not found in the Federal LCA Commons are exported from the NETL master database (currently still under development)
    Additionally, two inputs from the UKy flowsheet inventory are not available, namely: 1) oxalic acid and 2) D2EHPA
    To address this data limitation, we compile inventory and develop UPs for the missing inputs in openLCA
    The oxalic acid inventory is retrieved from ecoinvent database and the inventory for DEHPA is collected from the
    literature (https://doi.org/10.1016/j.resconrec.2022.106689)

2- Regarding methods, a new method 'PrOMMiS Impact Assessment Method' is developed that encompasses several impact categories
    - Acidification Potential                                       <- imported from TRACI 2.1
    - Cumulative Energy Demand                                      <- developed by compiling all the characterization factors from the Federal LCA Commons CED method
    - Cumulative Energy Demand - Non-renewable                      <- developed by only including the characterization factors for non-renewable resources
    - Cumulative Energy Demand - renewable                          <- developed by only including the characterization factors for renewable resources
    - Eutrophication Potential                                      <- imported from TRACI 2.1
    - Global Warming Potential [AR6, 100 yr]                        <- imported from TRACI 2.1
    - Global Warming Potential [AR6, 20 yr]                         <- imported from TRACI 2.1
    - Human Health - cancer                                         <- imported from TRACI 2.1
    - Human Health - non-cancer                                     <- imported from TRACI 2.1
    - Human Health - particulate matter                             <- imported from TRACI 2.1
    - Ozone Depletion Potential                                     <- imported from TRACI 2.1
    - Smog Formation Potential                                      <- imported from TRACI 2.1
    - Water Consumption (NETL)                                      <- developed by NETL - imported from TRACI 2.1 (NETL)


Questions and future considerations
------------------------------------
1- Is hotspot analysis a part of the final outcome?
    If not then the current model structure is finalize_df
    If yes, then this would require modeling the UKy flowsheet (for example)
    using process-based LCA rather than aggregating all the inventory in one unit process
    In the second case (yes), we might need to do minor changes to the model structure to
    allow modeling each process unit separately (something to keep in mind: in the case of
    processes producing co-products/by-products, this becomes exponentially more complex to
    solve - but definitely doable)

2- When creating processes, is it better to search for the process/
provider and then select a flow? Or is it better to select a flow and then search for relevant
providers (which is the current approach)?

3- Potential improvement in the future: the user gets to see a list of impact assessment methods
and select one for their analysis.

4- In the process description step, we can ask the user to enter different LCA descriptors and
then combine those to form the process description. This ensures the process has all the necessary
information: goal, scope, functional unit, assumptions, etc.

5- dislpay units in the list of flows

6- enable modifying flow unit (in lca_df_finalized) and adding conversion parameters
where needed when creating a flow (e.g., natural gas - from MJ to kg)

Needed improvements and other TODO's for the current deliverable (09/30/2025)
-----------------------------------------------------------------------------




New Technical Direction - EY25 - Q3/Q4 (11/10/2025)
---------------------------------------------------\

Info from TD
    * Establish a protocol for bi-directional communication between 
            PrOMMiS and openLCA - allow openLCA results to inform PrOMMiS optimization
                ** Supports constraint-based decision framework
                    My understanding, is that this should involve something simialr to the 
                    current PrOMMiS optimization framework
                        1- Setting Goal
                        2- Setting optimization parameter
                        3- Setting constraints
                        2 and 3 are probably what the TD note is trying to address
                            (e.g. "narrow the analysis scope in the context of practical considerations)
    * Deliverable should be in the form of a 
        ** methods memo
        ** demonstration on the UKy flowsheet
        ** test on other flowsheets

Notes on optimization model in PrOMMiS
    * Optiomization in prommis is done using nonlinear programming via IPOPT which
        ** updates unfixed design variables 
        ** to minimize the active Objective (e.g., minimize environmental impact)

    * the workflow consists of 
        ** unfixing decision variables (e.g., membrane length)
            {variable}.unfix()
        ** adding constraints using pyomo's 
            *** Expression({insert expression})
            *** Param(initialize={insert parameter}, mutable = True)
            *** Constraint(expr={insert conditional expression})
        ** adding objective
            Objective(expr = {insert variable such as cost}, sense = {e.g., minimize})

    * the above optimization elements can all be defined in a single function
        ** for exampple def build_optimization(...)
            to facilitate configuring the optimizer
        ** or this can be a populated automatically based on inputs in a 
            yaml configuration file

Potential additions/modifications to existing solution from previous PrOMMiS-LCA integration example
    * Create new function to configure the optimization solver
        variables, constraints, Objective
        ** the objective and constraint in this case can be complicated 
            *** which impact category?
            *** is cost included?
            *** if more than one impact category 
                weighing factors might be needed for the optimization equation
            *** if env impact + cost are included
                weighing factors might be needed for the optimization equation 
    * Add feature to create a parameter for each exchange
    * Create function to populate parameters after every iteration
        ** we can possibly include a feature - maybe an option for the user - to save 
            all the solutions for subsequent revision and/or data analysis
    * Create function to set allocation - which is very crucial for CM flowsheets
    * Create function to allow modeling displaced/avoided products
    * Modify existing script to allow the user define uncertainty distributions to exchanges
        Potential applications include
        ** Optimizing based on a statistical inference rather than a single deterministic value
        ** Allowing the user to run an uncertainty analysis for the final solution

Other necessary modifications to facilitate the reusability of the solution for other flowsheets
    * PrOMMiS data collection and conversion to LCA relevant units
        ** script should be limited to PrOMMiS output
        ** add function to enable the user to add exchanges as needed when running the model
            (without having to modify the code)
    * Exchange table development
        ** modify existing functions or create new function that allows users to add documentation for each
            exchange

FOQUS Installation
------------------

- FOQUS Documentation: https://foqus.readthedocs.io/en/latest/chapt_install/install_python.html

- led by NETL and in collaboration with other NL and units

- Purpose: connect different models
    * uncertainty
    * sensitivity


Ubuntu notes
------------
Installation steps (using both conda and pip)

    * conda create --name ccsi-foqus -c conda-forge python=3.12
    * conda activate ccsi-foqus
    
    Install FOQUS
    =============
    * conda install pyqt
    * git clone https://github.com/CCSI-Toolset/FOQUS
    * cd FOQUS
    * pip install -e .

    Install Psuade Lite 
    ===================
    * conda install --yes -c conda-forge -c CCSI-Toolset psuade-lite=1.9

    Install NLopt
    =============
    conda install -c conda-forge nlopt

    Install prommis
    ===============
    * pip install prommis
    * idaes get-extensions
    * pip install pyomo

    Install IPOPT
    =============
    * conda install -c conda-forge ipopt

    Install prommis-lca
    ===================
    * git clone https://github.com/KeyLogicLCA/lca-prommis.git
    * cd lca-prommis
    * pip install -e .

                Note:
                ====
                * openLCA-related libraries require python >= 3.11 
                    this was the main reason to move to linux/wsl
                    we can run prommis/foqus/olca in python 3.12
                * After resolving all dependency issues - one remained
                    * foqus     requires    matplotlib <=3.7.5 and >=3.6.0
                    * pymatgen  requires    matplotlib >=3.8

                    * Only solution --> downgrade matplotlib to 3.7.5
                                    --> downgrade pymatgen to 2022.x 
                                        to be compatible with matplotlib 3.7.5 

                                        Issue: pymatgen 2022.x requires Python 3.10

                BUT - since we're not plotting anything with Pymatgen - this dependency 
                    issue shouldn't cause any problems 
                
                SKIP THE DETAILS --> see final versions below
                ================
                matplotlib  3.10.7
                pymatgen    2024.11.13
                tabulate    0.9.0

    Install openLCA (TYLER CAN YOU PLEASE ADD THE COMMANDS for openLCA HERE? Please:) )
    ===============

Use and testing
===============

- Running foqus: 
    * Now that conda automatically starts - cd ccsi-foqus
    * run:  foqus
        or
            foqus &     #this allows the user to regain control of their terminal while using foqus 

- Test
    * Create a node in foqus - call it 't' and initiate it at '0'
    * Run the below script:
    
    """
        import pandas as pd
        import olca_schema as olca
        from netlolca.NetlOlca import NetlOlca
        import prommis.uky.uky_flowsheet as uky 
        import src as lca_prommis

        m, _ = uky.main() 

        prommis_data = lca_prommis.data_lca.get_lca_df(m)

        df = lca_prommis.convert_lca.convert_flows_to_lca_units(prommis_data, hours=1, mol_to_kg=True, water_unit='m3')

        REO_list = [
            "Yttrium Oxide",
            "Lanthanum Oxide",
            "Cerium Oxide",
            "Praseodymium Oxide",
            "Neodymium Oxide",
            "Samarium Oxide",
            "Gadolinium Oxide",
            "Dysprosium Oxide",
        ]

        df = lca_prommis.final_lca.merge_flows(df, merge_source='Solid Feed', new_flow_name='374 ppm REO Feed', value_2_merge=REO_list)


        df = lca_prommis.final_lca.merge_flows(df, merge_source='Roaster Product', new_flow_name='73.4% REO Product')


        df = lca_prommis.final_lca.merge_flows(df, merge_source='Wastewater', new_flow_name='Wastewater', merge_column='Category') 

        df = lca_prommis.final_lca.merge_flows(df, merge_source='Solid Waste', new_flow_name='Solid Waste', merge_column='Category') 

        finalized_df = lca_prommis.final_lca.finalize_df(
                df=df, 
                reference_flow='73.4% REO Product', 
                reference_source='Roaster Product',
                water_type='raw fresh water'
            )

        f["t"] = finalized_df["LCA_Amount"].iloc[0]
    """
    * Running the script should update the value of the output 't' from 0 to 1043142.7399884746

DFO documentation
-----------------


NLopt documentation
-------------------


